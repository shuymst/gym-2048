{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "import gymnasium as gym\n",
    "import gym_2048\n",
    "from gym_2048.wrappers import ConvObservation, PrintScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(18, 256, kernel_size=2, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(256, 512, kernel_size=2, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        self.fc_out = nn.Linear(256, 4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.conv1(x))\n",
    "        h = F.relu(self.conv2(h)).view(-1, 2048)\n",
    "        h = F.relu(self.fc1(h))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        out = self.fc_out(h)\n",
    "        return out\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(18, 256, kernel_size=2, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(256, 512, kernel_size=2, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        self.fc_out = nn.Linear(256, 4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.conv1(x))\n",
    "        h = F.relu(self.conv2(h)).view(-1, 2048)\n",
    "        h = F.relu(self.fc1(h))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        out = self.fc_out(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gumbel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        device: torch.device,\n",
    "        critic_lr: float,\n",
    "        actor_lr: float,\n",
    "        n_envs: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.n_envs = n_envs\n",
    "        \n",
    "        self.actor = PolicyNetwork().to(self.device)\n",
    "        self.critic = QNetwork().to(self.device)\n",
    "\n",
    "        self.actor_optim = optim.Adam(params=self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optim = optim.Adam(params=self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "        self.c_visit = 50\n",
    "        self.c_scale = 0.1\n",
    "\n",
    "    def forward(self, x: np.ndarray):\n",
    "        x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
    "        q_values = self.critic(x)\n",
    "        action_logits = self.actor(x)\n",
    "        return (q_values, action_logits)\n",
    "\n",
    "    def select_action(self, x: np.ndarray, legal_actions):\n",
    "\n",
    "        batch_size = len(x)\n",
    "        q_values, action_logits = self.forward(x)\n",
    "        selected_actions = torch.zeros(size=(batch_size,), dtype=torch.int32, device=self.device)\n",
    "        action_logprobs = torch.zeros(size=(batch_size,), dtype=torch.float32, device=self.device)\n",
    "        selecte_q_values = torch.zeros(size=(batch_size,), dtype=torch.float32, device=self.device)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            n_legal_actions = len(legal_actions[i])\n",
    "            gumbel_noise = torch.tensor(np.random.gumbel(size=(n_legal_actions,)), device=self.device)\n",
    "            logits = action_logits[i][legal_actions[i]]\n",
    "            qs = q_values[i][legal_actions[i]]\n",
    "            normalized_q_values = qs / (torch.max(qs) - torch.min(qs))\n",
    "            action_idx = torch.argmax(gumbel_noise + logits + self.c_visit * self.c_scale * normalized_q_values)\n",
    "            \n",
    "            selected_actions[i] = legal_actions[i][action_idx]\n",
    "            action_pd = torch.distributions.Categorical(logits=action_logits[i])\n",
    "            action_logprobs[i] = action_pd.log_prob(selected_actions[i])\n",
    "            selecte_q_values[i] = q_values[i][selected_actions[i]]\n",
    "\n",
    "        return selected_actions, action_logprobs, selecte_q_values\n",
    "    \n",
    "    def get_losses(\n",
    "        self,\n",
    "        rewards,\n",
    "        action_log_probs,\n",
    "        q_value_preds,\n",
    "        masks,\n",
    "        device,\n",
    "    ):\n",
    "        T = len(rewards)\n",
    "        td_errors = torch.zeros(T, self.n_envs, device=device)\n",
    "        for t in reversed(range(T-1)):\n",
    "            td_error = (rewards[t] + masks[t] * q_value_preds[t+1]).detach() - q_value_preds[t]\n",
    "            td_errors[t] = td_error\n",
    "        \n",
    "        critic_loss = td_errors.pow(2).mean()\n",
    "        actor_loss = -action_log_probs.mean()\n",
    "\n",
    "        return critic_loss, actor_loss\n",
    "    \n",
    "    def update_parameters(self, critic_loss, actor_loss):\n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_envs = 10\n",
    "n_updates = 10000\n",
    "n_steps_per_update = 128\n",
    "\n",
    "actor_lr = 0.001\n",
    "critic_lr = 0.005\n",
    "\n",
    "envs = gym.vector.make(\"TwentyFortyEight-v0\", num_envs=n_envs, wrappers=ConvObservation)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "agent = Gumbel(device, critic_lr, actor_lr, n_envs)\n",
    "\n",
    "envs_wrapper = PrintScores(envs, deque_size=n_envs * n_updates)\n",
    "\n",
    "critic_losses = []\n",
    "actor_losses = []\n",
    "entropies = []\n",
    "\n",
    "for sample_phase in tqdm(range(n_updates)):\n",
    "    ep_q_value_preds = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "    ep_rewards = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "    ep_action_log_probs = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "    masks = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "\n",
    "    if sample_phase == 0:\n",
    "        states, infos = envs_wrapper.reset(seed=42)\n",
    "    \n",
    "    for step in range(n_steps_per_update):\n",
    "        actions, action_log_probs, q_value_preds = agent.select_action(states, infos[\"legal actions\"])\n",
    "\n",
    "        states, rewards, terminated, truncated, infos = envs_wrapper.step(actions.cpu().numpy())\n",
    "\n",
    "        ep_q_value_preds[step] = torch.squeeze(q_value_preds)\n",
    "        ep_rewards[step] = torch.tensor(rewards, device=device)\n",
    "        ep_action_log_probs[step] = action_log_probs\n",
    "        masks[step] = torch.tensor([not term for term in terminated])\n",
    "    \n",
    "    critic_loss, actor_loss = agent.get_losses(\n",
    "        ep_rewards,\n",
    "        ep_action_log_probs,\n",
    "        ep_q_value_preds,\n",
    "        masks,\n",
    "        device,\n",
    "    )\n",
    "\n",
    "    agent.update_parameters(critic_loss, actor_loss)\n",
    "    critic_losses.append(critic_loss.detach().cpu().numpy())\n",
    "    actor_losses.append(actor_loss.detach().cpu().numpy())\n",
    "\n",
    "rolling_length = 20\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 5))\n",
    "\n",
    "# episode return\n",
    "axs[0][0].set_title(\"Episode Returns\")\n",
    "episode_returns_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(envs_wrapper.return_queue).flatten(),\n",
    "        np.ones(rolling_length),\n",
    "        mode=\"valid\",\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[0][0].plot(\n",
    "    np.arange(len(episode_returns_moving_average)) / n_envs,\n",
    "    episode_returns_moving_average,\n",
    ")\n",
    "axs[0][0].set_xlabel(\"Number of episodes\")\n",
    "\n",
    "# entropy\n",
    "axs[1][0].set_title(\"Entropy\")\n",
    "entropy_moving_average = (\n",
    "    np.convolve(np.array(entropies), np.ones(rolling_length), mode=\"valid\")\n",
    "    / rolling_length\n",
    ")\n",
    "axs[1][0].plot(entropy_moving_average)\n",
    "axs[1][0].set_xlabel(\"Number of updates\")\n",
    "\n",
    "\n",
    "# critic loss\n",
    "axs[0][1].set_title(\"Critic Loss\")\n",
    "critic_losses_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(critic_losses).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[0][1].plot(critic_losses_moving_average)\n",
    "axs[0][1].set_xlabel(\"Number of updates\")\n",
    "\n",
    "\n",
    "# actor loss\n",
    "axs[1][1].set_title(\"Actor Loss\")\n",
    "actor_losses_moving_average = (\n",
    "    np.convolve(np.array(actor_losses).flatten(), np.ones(rolling_length), mode=\"valid\")\n",
    "    / rolling_length\n",
    ")\n",
    "axs[1][1].plot(actor_losses_moving_average)\n",
    "axs[1][1].set_xlabel(\"Number of updates\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"gumbel.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Afterstateモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AfterstatePolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AfterstatePolicyNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(18, 256, kernel_size=2, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(256, 512, kernel_size=2, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        self.fc_out = nn.Linear(256, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.conv1(x))\n",
    "        h = F.relu(self.conv2(h)).view(-1, 2048)\n",
    "        h = F.relu(self.fc1(h))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        out = self.fc_out(h)\n",
    "        return out\n",
    "\n",
    "class AfterstateValueNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AfterstateValueNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(18, 256, kernel_size=2, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(256, 512, kernel_size=2, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        self.fc_out = nn.Linear(256, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.conv1(x))\n",
    "        h = F.relu(self.conv2(h)).view(-1, 2048)\n",
    "        h = F.relu(self.fc1(h))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        out = self.fc_out(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_conv_feature(inputs):\n",
    "    \"\"\"\n",
    "    inputs: np.ndarray of shape(n, 4, 4)\n",
    "    output: np.ndarray of shape(n, 18, 4, 4)\n",
    "    \"\"\"\n",
    "    conv_feature = np.zeros(shape=(len(inputs), 18, 4, 4))\n",
    "    \n",
    "    for i in range(len(inputs)):\n",
    "        for x in range(4):\n",
    "            for y in range(4):\n",
    "                conv_feature[i][inputs[i][x][y]][x][y] = 1.0\n",
    "    \n",
    "    return conv_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AfterstateGumbel(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        device: torch.device,\n",
    "        critic_lr: float,\n",
    "        actor_lr: float,\n",
    "        n_envs: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.n_envs = n_envs\n",
    "        \n",
    "        self.actor = AfterstatePolicyNetwork().to(self.device)\n",
    "        self.critic = AfterstateValueNetwork().to(self.device)\n",
    "\n",
    "        self.actor_optim = optim.Adam(params=self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optim = optim.Adam(params=self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "        self.c_visit = 50\n",
    "        self.c_scale = 0.1\n",
    "\n",
    "    def forward(self, afterstates: np.ndarray):\n",
    "        concatenated_afterstates = np.concatenate(afterstates, axis=0)\n",
    "        conv_afterstates = torch.tensor(make_conv_feature(concatenated_afterstates), dtype=torch.float32, device=self.device)\n",
    "        afterstate_values = self.critic(conv_afterstates)\n",
    "        afterstate_policy_logits = self.actor(conv_afterstates)\n",
    "        return (afterstate_values.squeeze(), afterstate_policy_logits.squeeze())\n",
    "\n",
    "    def select_action(self, legal_actions, afterstates, afterstate_rewards):\n",
    "        \n",
    "        batch_size = len(legal_actions)\n",
    "        afterstate_values, afterstate_policy_logits = self.forward(afterstates)\n",
    "        q_values = afterstate_values + torch.tensor(np.concatenate(afterstate_rewards, axis=0), dtype=torch.float32, device=self.device)\n",
    "        gumbel_noise = torch.tensor(np.random.gumbel(size=(len(afterstate_values),)), device=self.device)\n",
    "      \n",
    "        selected_actions = torch.zeros(size=(batch_size,), dtype=torch.int32, device=self.device)\n",
    "        action_logprobs = torch.zeros(size=(batch_size,), dtype=torch.float32, device=self.device)\n",
    "        selected_q_values = torch.zeros(size=(batch_size,), dtype=torch.float32, device=self.device)\n",
    "\n",
    "        idx_cnt = 0\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            n_legal_actions = len(legal_actions[i])\n",
    "            noise = gumbel_noise[idx_cnt:idx_cnt+n_legal_actions]\n",
    "            policy_logits = afterstate_policy_logits[idx_cnt:idx_cnt+n_legal_actions]\n",
    "            qs = q_values[idx_cnt:idx_cnt+n_legal_actions]\n",
    "            normalized_q_values = qs / (torch.max(qs) - torch.min(qs))\n",
    "\n",
    "            action_idx = torch.argmax(noise + policy_logits + self.c_visit * self.c_scale * normalized_q_values)\n",
    "\n",
    "            selected_actions[i] = legal_actions[i][action_idx]\n",
    "            action_pd = torch.distributions.Categorical(logits=policy_logits)\n",
    "            action_logprobs[i] = action_pd.log_prob(action_idx)\n",
    "            selected_q_values[i] = qs[action_idx]\n",
    "\n",
    "            idx_cnt += n_legal_actions\n",
    "\n",
    "        return selected_actions, action_logprobs, selected_q_values\n",
    "    \n",
    "    def get_losses(\n",
    "        self,\n",
    "        rewards,\n",
    "        action_log_probs,\n",
    "        q_value_preds,\n",
    "        masks,\n",
    "        device,\n",
    "    ):\n",
    "        T = len(rewards)\n",
    "        td_errors = torch.zeros(T, self.n_envs, device=device)\n",
    "        for t in reversed(range(T-1)):\n",
    "            td_error = (rewards[t] + masks[t] * q_value_preds[t+1]).detach() - q_value_preds[t]\n",
    "            td_errors[t] = td_error\n",
    "        \n",
    "        critic_loss = td_errors.pow(2).mean()\n",
    "        actor_loss = -action_log_probs.mean()\n",
    "\n",
    "        return critic_loss, actor_loss\n",
    "    \n",
    "    def update_parameters(self, critic_loss, actor_loss):\n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[592.]\n",
      "[848.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/10000 [00:01<4:09:55,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1388.]\n",
      "[1396.]\n",
      "[1432.]\n",
      "[668.]\n",
      "[1964.]\n",
      "[2204.]\n",
      "[2756.]\n",
      "[1328.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/10000 [00:02<3:47:28,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3140.]\n",
      "[3484.]\n",
      "[1372.]\n",
      "[1340.]\n",
      "[1768.]\n",
      "[616.]\n",
      "[544.]\n",
      "[1220.]\n",
      "[1660.]\n",
      "[ 580. 1232.]\n",
      "[2720.]\n",
      "[2388.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/10000 [00:04<3:41:57,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[712.]\n",
      "[720.]\n",
      "[1364.]\n",
      "[1316. 2336.]\n",
      "[1356.]\n",
      "[1264.]\n",
      "[1408.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/10000 [00:05<3:39:09,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[888.]\n",
      "[580.]\n",
      "[704.]\n",
      "[532.]\n",
      "[1128.]\n",
      "[2364.]\n",
      "[2816.]\n",
      "[712.]\n",
      "[1060.]\n",
      "[1432.]\n",
      "[304.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/10000 [00:06<3:37:42,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[392.]\n",
      "[1028.]\n",
      "[1252.]\n",
      "[1384.]\n",
      "[1344.]\n",
      "[1640.]\n",
      "[1868.]\n",
      "[656.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/10000 [00:07<3:36:23,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1352.]\n",
      "[2432.]\n",
      "[580.]\n",
      "[1180.]\n",
      "[984.]\n",
      "[2676.]\n",
      "[2756.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/10000 [00:09<3:36:04,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2316.]\n",
      "[2612.]\n",
      "[2104.]\n",
      "[1572.]\n",
      "[1324.]\n",
      "[3332.]\n",
      "[2640.]\n",
      "[1636.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 9/10000 [00:11<3:36:34,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3212.]\n",
      "[3604.]\n",
      "[3092.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/10000 [00:13<3:36:40,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7016.]\n",
      "[1504.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 11/10000 [00:14<3:35:42,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1780.]\n",
      "[6528.]\n",
      "[6544.]\n",
      "[7344.]\n",
      "[7924.]\n",
      "[7992.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 12/10000 [00:15<3:34:39,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3560.]\n",
      "[3336.]\n",
      "[12384.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 13/10000 [00:16<3:34:25,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2644.]\n",
      "[1620.]\n",
      "[3840.]\n",
      "[3244.]\n",
      "[3180.]\n",
      "[3184.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 14/10000 [00:18<3:34:03,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4420.]\n",
      "[3048.]\n",
      "[1768.]\n",
      "[2560.]\n",
      "[3420.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 15/10000 [00:19<3:33:47,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4760.]\n",
      "[1528.]\n",
      "[3560.]\n",
      "[2840.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 16/10000 [00:20<3:34:11,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1576.]\n",
      "[1052.]\n",
      "[2092.]\n",
      "[4224.]\n",
      "[1476.]\n",
      "[4096.]\n",
      "[3436. 2400.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17/10000 [00:22<3:34:31,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1748.]\n",
      "[2488.]\n",
      "[2624.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 18/10000 [00:23<3:34:54,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3460.]\n",
      "[3716.]\n",
      "[2984.]\n",
      "[3832.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 19/10000 [00:24<3:34:03,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3780.]\n",
      "[748.]\n",
      "[3748.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 19/10000 [00:25<3:43:39,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5116.]\n",
      "[4012.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m     states, infos \u001b[39m=\u001b[39m envs_wrapper\u001b[39m.\u001b[39mreset(seed\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_steps_per_update):\n\u001b[0;32m---> 30\u001b[0m     actions, action_log_probs, q_value_preds \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mselect_action(infos[\u001b[39m\"\u001b[39;49m\u001b[39mlegal actions\u001b[39;49m\u001b[39m\"\u001b[39;49m], infos[\u001b[39m\"\u001b[39;49m\u001b[39mafterstates\u001b[39;49m\u001b[39m\"\u001b[39;49m], infos[\u001b[39m\"\u001b[39;49m\u001b[39mrewards\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m     32\u001b[0m     states, rewards, terminated, truncated, infos \u001b[39m=\u001b[39m envs_wrapper\u001b[39m.\u001b[39mstep(actions\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m     34\u001b[0m     ep_q_value_preds[step] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze(q_value_preds)\n",
      "Cell \u001b[0;32mIn[28], line 53\u001b[0m, in \u001b[0;36mAfterstateGumbel.select_action\u001b[0;34m(self, legal_actions, afterstates, afterstate_rewards)\u001b[0m\n\u001b[1;32m     51\u001b[0m selected_actions[i] \u001b[39m=\u001b[39m legal_actions[i][action_idx]\n\u001b[1;32m     52\u001b[0m action_pd \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdistributions\u001b[39m.\u001b[39mCategorical(logits\u001b[39m=\u001b[39mpolicy_logits)\n\u001b[0;32m---> 53\u001b[0m action_logprobs[i] \u001b[39m=\u001b[39m action_pd\u001b[39m.\u001b[39;49mlog_prob(action_idx)\n\u001b[1;32m     54\u001b[0m selected_q_values[i] \u001b[39m=\u001b[39m qs[action_idx]\n\u001b[1;32m     56\u001b[0m idx_cnt \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m n_legal_actions\n",
      "File \u001b[0;32m~/code/gym-2048/.venv/lib/python3.10/site-packages/torch/distributions/categorical.py:123\u001b[0m, in \u001b[0;36mCategorical.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_prob\u001b[39m(\u001b[39mself\u001b[39m, value):\n\u001b[1;32m    122\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_args:\n\u001b[0;32m--> 123\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_sample(value)\n\u001b[1;32m    124\u001b[0m     value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    125\u001b[0m     value, log_pmf \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbroadcast_tensors(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogits)\n",
      "File \u001b[0;32m~/code/gym-2048/.venv/lib/python3.10/site-packages/torch/distributions/distribution.py:298\u001b[0m, in \u001b[0;36mDistribution._validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[39massert\u001b[39;00m support \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 298\u001b[0m valid \u001b[39m=\u001b[39m support\u001b[39m.\u001b[39;49mcheck(value)\n\u001b[1;32m    299\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[1;32m    300\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    301\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpected value argument \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    302\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_envs = 10\n",
    "n_updates = 10000\n",
    "n_steps_per_update = 128\n",
    "\n",
    "actor_lr = 0.001\n",
    "critic_lr = 0.005\n",
    "\n",
    "envs = gym.vector.make(\"TwentyFortyEight-v0\", num_envs=n_envs, afterstate=True)\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "agent = AfterstateGumbel(device, critic_lr, actor_lr, n_envs)\n",
    "\n",
    "envs_wrapper = PrintScores(envs)\n",
    "\n",
    "critic_losses = []\n",
    "actor_losses = []\n",
    "entropies = []\n",
    "\n",
    "for sample_phase in tqdm(range(n_updates)):\n",
    "    ep_q_value_preds = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "    ep_rewards = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "    ep_action_log_probs = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "    masks = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "\n",
    "    if sample_phase == 0:\n",
    "        states, infos = envs_wrapper.reset(seed=42)\n",
    "    \n",
    "    for step in range(n_steps_per_update):\n",
    "        actions, action_log_probs, q_value_preds = agent.select_action(infos[\"legal actions\"], infos[\"afterstates\"], infos[\"rewards\"])\n",
    "\n",
    "        states, rewards, terminated, truncated, infos = envs_wrapper.step(actions.cpu().numpy())\n",
    "\n",
    "        ep_q_value_preds[step] = torch.squeeze(q_value_preds)\n",
    "        ep_rewards[step] = torch.tensor(rewards, device=device)\n",
    "        ep_action_log_probs[step] = action_log_probs\n",
    "        masks[step] = torch.tensor([not term for term in terminated])\n",
    "    \n",
    "    critic_loss, actor_loss = agent.get_losses(\n",
    "        ep_rewards,\n",
    "        ep_action_log_probs,\n",
    "        ep_q_value_preds,\n",
    "        masks,\n",
    "        device,\n",
    "    )\n",
    "\n",
    "    agent.update_parameters(critic_loss, actor_loss)\n",
    "    critic_losses.append(critic_loss.detach().cpu().numpy())\n",
    "    actor_losses.append(actor_loss.detach().cpu().numpy())\n",
    "\n",
    "rolling_length = 20\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(12, 5))\n",
    "\n",
    "# episode return\n",
    "axs[0][0].set_title(\"Episode Returns\")\n",
    "episode_returns_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(envs_wrapper.return_queue).flatten(),\n",
    "        np.ones(rolling_length),\n",
    "        mode=\"valid\",\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[0][0].plot(\n",
    "    np.arange(len(episode_returns_moving_average)) / n_envs,\n",
    "    episode_returns_moving_average,\n",
    ")\n",
    "axs[0][0].set_xlabel(\"Number of episodes\")\n",
    "\n",
    "# entropy\n",
    "axs[1][0].set_title(\"Entropy\")\n",
    "entropy_moving_average = (\n",
    "    np.convolve(np.array(entropies), np.ones(rolling_length), mode=\"valid\")\n",
    "    / rolling_length\n",
    ")\n",
    "axs[1][0].plot(entropy_moving_average)\n",
    "axs[1][0].set_xlabel(\"Number of updates\")\n",
    "\n",
    "\n",
    "# critic loss\n",
    "axs[0][1].set_title(\"Critic Loss\")\n",
    "critic_losses_moving_average = (\n",
    "    np.convolve(\n",
    "        np.array(critic_losses).flatten(), np.ones(rolling_length), mode=\"valid\"\n",
    "    )\n",
    "    / rolling_length\n",
    ")\n",
    "axs[0][1].plot(critic_losses_moving_average)\n",
    "axs[0][1].set_xlabel(\"Number of updates\")\n",
    "\n",
    "\n",
    "# actor loss\n",
    "axs[1][1].set_title(\"Actor Loss\")\n",
    "actor_losses_moving_average = (\n",
    "    np.convolve(np.array(actor_losses).flatten(), np.ones(rolling_length), mode=\"valid\")\n",
    "    / rolling_length\n",
    ")\n",
    "axs[1][1].plot(actor_losses_moving_average)\n",
    "axs[1][1].set_xlabel(\"Number of updates\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"gumbel.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
