{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "import gymnasium as gym\n",
    "import gym_2048\n",
    "from gym_2048.wrappers.conv_observation import ConvObservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(18, 256, kernel_size=2, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(256, 512, kernel_size=2, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        self.fc_out = nn.Linear(256, 4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.conv1(x))\n",
    "        h = F.relu(self.conv2(h)).view(-1, 2048)\n",
    "        h = F.relu(self.fc1(h))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        out = self.fc_out(h)\n",
    "        return out\n",
    "\n",
    "class ValueNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(18, 256, kernel_size=2, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(256, 512, kernel_size=2, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 256)\n",
    "        self.fc_out = nn.Linear(256, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.conv1(x))\n",
    "        h = F.relu(self.conv2(h)).view(-1, 2048)\n",
    "        h = F.relu(self.fc1(h))\n",
    "        h = F.relu(self.fc2(h))\n",
    "        out = self.fc_out(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        device: torch.device,\n",
    "        critic_lr: float,\n",
    "        actor_lr: float,\n",
    "        n_envs: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.n_envs = n_envs\n",
    "        \n",
    "        self.actor = PolicyNetwork()\n",
    "        self.critic = ValueNetwork()\n",
    "\n",
    "        self.actor_optim = optim.Adam(params=self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optim = optim.Adam(params=self.critic.parameters(), lr=critic_lr)\n",
    "\n",
    "    def forward(self, x: np.ndarray):\n",
    "        x = torch.tensor(x, dtype=torch.float32).to(self.device)\n",
    "        state_values = self.critic(x)\n",
    "        action_logits = self.actor(x)\n",
    "        return (state_values, action_logits)\n",
    "\n",
    "    def select_action(self, x: np.ndarray, legal_actions):\n",
    "        batch_size = len(x)\n",
    "        state_values, action_logits = self.forward(x)\n",
    "\n",
    "        selected_actions = torch.zeros(size=(batch_size,), dtype=torch.int32, device=self.device)\n",
    "        action_logprobs = torch.zeros(size=(batch_size,), dtype=torch.float32, device=self.device)\n",
    "        entropy = torch.zeros(size=(batch_size,), dtype=torch.float32, device=self.device)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            action_pd = torch.distributions.Categorical(logits=action_logits[i][legal_actions[i]])\n",
    "            selected_actions[i] = action_pd.sample()\n",
    "            action_logprobs[i] = action_pd.log_prob(selected_actions[i])\n",
    "            entropy[i] = action_pd.entropy()\n",
    "\n",
    "        return selected_actions, action_logprobs, state_values, entropy\n",
    "    \n",
    "    def get_losses(\n",
    "        self,\n",
    "        rewards,\n",
    "        action_log_probs,\n",
    "        value_preds,\n",
    "        entropy,\n",
    "        masks,\n",
    "        gamma,\n",
    "        lam,\n",
    "        ent_coef,\n",
    "        device,\n",
    "    ):\n",
    "        T = len(rewards)\n",
    "        advantages = torch.zeros(T, self.n_envs, device=device)\n",
    "\n",
    "        gae = 0.0\n",
    "        for t in reversed(range(T-1)):\n",
    "            td_error = rewards[t] + gamma * masks[t] * value_preds[t+1] - value_preds[t]\n",
    "            gae = td_error + gamma * lam * masks[t] * gae\n",
    "            advantages[t] = gae\n",
    "        \n",
    "        critic_loss = advantages.pow(2).mean()\n",
    "        actor_loss = -(advantages.detach() * action_log_probs).mean() - ent_coef * entropy.mean()\n",
    "\n",
    "        return critic_loss, actor_loss\n",
    "    \n",
    "    def update_parameters(self, critic_loss, actor_loss):\n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_envs = 10\n",
    "n_updates = 10000\n",
    "n_steps_per_update = 128\n",
    "\n",
    "gamma = 0.999\n",
    "lam = 0.95\n",
    "ent_coef = 0.01\n",
    "actor_lr = 0.001\n",
    "critic_lr = 0.005\n",
    "\n",
    "envs = gym.vector.make(\"TwentyFortyEight-v0\", num_envs=n_envs, wrappers=ConvObservation)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "agent = A2C(device, critic_lr, actor_lr, n_envs)\n",
    "\n",
    "envs_wrapper = gym.wrappers.RecordEpisodeStatistics(envs, deque_size=n_envs * n_updates)\n",
    "\n",
    "critic_losses = []\n",
    "actor_losses = []\n",
    "entropies = []\n",
    "\n",
    "for sample_phase in tqdm(range(n_updates)):\n",
    "    ep_value_preds = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "    ep_rewards = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "    ep_action_log_probs = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "    masks = torch.zeros(n_steps_per_update, n_envs, device=device)\n",
    "\n",
    "    if sample_phase == 0:\n",
    "        states, infos = envs_wrapper.reset(seed=42)\n",
    "    \n",
    "    for step in range(n_steps_per_update):\n",
    "        actions, action_log_probs, state_value_preds, entropy = agent.select_action(states, infos[\"legal actions\"])\n",
    "\n",
    "        states, rewards, terminated, truncated, infos = envs_wrapper.step(actions.cpu().numpy())\n",
    "\n",
    "        ep_value_preds[step] = torch.squeeze(state_value_preds)\n",
    "        ep_rewards[step] = torch.tensor(rewards, device=device)\n",
    "        ep_action_log_probs[step] = action_log_probs\n",
    "        masks[step] = torch.tensor([not term for term in terminated])\n",
    "    \n",
    "    critic_loss, actor_loss = agent.get_losses(\n",
    "        ep_rewards,\n",
    "        ep_action_log_probs,\n",
    "        ep_value_preds,\n",
    "        entropy,\n",
    "        masks,\n",
    "        gamma,\n",
    "        lam,\n",
    "        ent_coef,\n",
    "        device,\n",
    "    )\n",
    "\n",
    "    agent.update_parameters(critic_loss, actor_loss)\n",
    "\n",
    "    critic_losses.append(critic_loss.detach().cpu().numpy())\n",
    "    actor_losses.append(actor_loss.detach().cpu().numpy())\n",
    "    entropies.append(entropy.detach().mean().cpu().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
